
     Program PWSCF v.5.3.0 (svn rev. 12238M) starts on 23Mar2016 at 10:18:47 

     This program is part of the open-source Quantum ESPRESSO suite
     for quantum simulation of materials; please cite
         "P. Giannozzi et al., J. Phys.:Condens. Matter 21 395502 (2009);
          URL http://www.quantum-espresso.org", 
     in publications or presentations arising from this work. More details at
     http://www.quantum-espresso.org/quote

     Parallel version (MPI), running on     2 processors
     R & G space division:  proc/nbgrp/npool/nimage =       2
     Waiting for input...
     Reading input from standard input
     Message from routine read_cards :
     DEPRECATED: no units specified in ATOMIC_POSITIONS card
     Message from routine read_cards :
     ATOMIC_POSITIONS: units set to alat

     Current dimensions of program PWSCF are:
     Max number of different atomic species (ntypx) = 10
     Max number of k-points (npk) =  40000
     Max angular momentum in pseudopotentials (lmaxx) =  3

     Subspace diagonalization in iterative solution of the eigenvalue problem:
     a serial algorithm will be used

 
     Parallelization info
     --------------------
     sticks:   dense  smooth     PW     G-vecs:    dense   smooth      PW
     Min         126     126     42                 1365     1365     264
     Max         127     127     43                 1368     1368     267
     Sum         253     253     85                 2733     2733     531
 


     bravais-lattice index     =            2
     lattice parameter (alat)  =      10.2000  a.u.
     unit-cell volume          =     265.3020 (a.u.)^3
     number of atoms/cell      =            2
     number of atomic types    =            1
     number of electrons       =         8.00
     number of Kohn-Sham states=            4
     kinetic-energy cutoff     =      18.0000  Ry
     charge density cutoff     =      72.0000  Ry
     convergence threshold     =      1.0E-08
     mixing beta               =       0.7000
     number of iterations used =            8  plain     mixing
     Exchange-correlation      =  SLA  PZ   NOGX NOGC ( 1  1  0  0 0 0)

     celldm(1)=  10.200000  celldm(2)=   0.000000  celldm(3)=   0.000000
     celldm(4)=   0.000000  celldm(5)=   0.000000  celldm(6)=   0.000000

     crystal axes: (cart. coord. in units of alat)
               a(1) = (  -0.500000   0.000000   0.500000 )  
               a(2) = (   0.000000   0.500000   0.500000 )  
               a(3) = (  -0.500000   0.500000   0.000000 )  

     reciprocal axes: (cart. coord. in units 2 pi/alat)
               b(1) = ( -1.000000 -1.000000  1.000000 )  
               b(2) = (  1.000000  1.000000  1.000000 )  
               b(3) = ( -1.000000  1.000000 -1.000000 )  


     PseudoPot. # 1 for Si read from file:
     /home/nvarini/espresso_hdf5/espresso/pseudo/Si.pz-vbc.UPF
     MD5 check sum: 6dfa03ddd5817404712e03e4d12deb78
     Pseudo is Norm-conserving, Zval =  4.0
     Generated by new atomic code, or converted to UPF format
     Using radial grid of  431 points,  2 beta functions with: 
                l(1) =   0
                l(2) =   1

     atomic species   valence    mass     pseudopotential
        Si             4.00    28.08600     Si( 1.00)

     48 Sym. Ops., with inversion, found (24 have fractional translation)



   Cartesian axes

     site n.     atom                  positions (alat units)
         1           Si  tau(   1) = (   0.0000000   0.0000000   0.0000000  )
         2           Si  tau(   2) = (   0.2500000   0.2500000   0.2500000  )

     number of k points=    10
                       cart. coord. in units 2pi/alat
        k(    1) = (   0.1250000   0.1250000   0.1250000), wk =   0.0625000
        k(    2) = (   0.1250000   0.1250000   0.3750000), wk =   0.1875000
        k(    3) = (   0.1250000   0.1250000   0.6250000), wk =   0.1875000
        k(    4) = (   0.1250000   0.1250000   0.8750000), wk =   0.1875000
        k(    5) = (   0.1250000   0.3750000   0.3750000), wk =   0.1875000
        k(    6) = (   0.1250000   0.3750000   0.6250000), wk =   0.3750000
        k(    7) = (   0.1250000   0.3750000   0.8750000), wk =   0.3750000
        k(    8) = (   0.1250000   0.6250000   0.6250000), wk =   0.1875000
        k(    9) = (   0.3750000   0.3750000   0.3750000), wk =   0.0625000
        k(   10) = (   0.3750000   0.3750000   0.6250000), wk =   0.1875000

     Dense  grid:     2733 G-vectors     FFT dimensions: (  20,  20,  20)

     Largest allocated arrays     est. size (Mb)     dimensions
        Kohn-Sham Wavefunctions         0.01 Mb     (     181,    4)
        NL pseudopotentials             0.02 Mb     (     181,    8)
        Each V/rho on FFT grid          0.06 Mb     (    4000)
        Each G-vector array             0.01 Mb     (    1365)
        G-vector shells                 0.00 Mb     (      65)
     Largest temporary arrays     est. size (Mb)     dimensions
        Auxiliary wavefunctions         0.04 Mb     (     181,   16)
        Each subspace H/S matrix        0.00 Mb     (      16,   16)
        Each <psi_i|beta_j> matrix      0.00 Mb     (       8,    4)
        Arrays for rho mixing           0.49 Mb     (    4000,    8)

     Initial potential from superposition of free atoms

     starting charge    7.99901, renormalised to    8.00000
     Starting wfc are    8 randomized atomic wfcs
HDF5-DIAG: Error detected in HDF5 (1.8.16) HDF5-DIAG: Error detected in HDF5 (1.8.16) MPI-process 1:
  #000: H5Dio.c line 271 in H5Dwrite(): can't prepare for writing data
    major: Dataset
    minor: Write failed
  #001: H5Dio.c line 352 in H5D__pre_write(): can't write data
    major: Dataset
    minor: Write failed
  #002: H5Dio.c line 789 in H5D__write(): can't write data
    major: Dataset
    minor: Write failed
  #003: H5Dmpio.c line 529 in H5D__contig_collective_write(): couldn't finish shared collective MPI-IO
    major: Low-level I/O
    minor: Write failed
  #004: H5Dmpio.c line 1399 in H5D__inter_collective_io(): couldn't finish collective MPI-IO
    major: Low-level I/O
    minor: Can't get value
  #005: H5Dmpio.c line 1443 in H5D__final_collective_io(): optimized write failed
    major: Dataset
    minor: Write failed
  #006: H5Dmpio.c line 297 in H5D__mpio_select_write(): can't finish collective parallel write
    major: Low-level I/O
    minor: Write failed
  #007: H5Fio.c line 171 in H5F_block_write(): write through metadata accumulator failed
    major: Low-level I/O
    minor: Write failed
  #008: H5Faccum.c line 825 in H5F__accum_write(): file write failed
    major: Low-level I/O
    minor: Write failed
  #009: H5FDint.c line 256 in H5FD_write(): addr overflow, addr = 18446744073709551615, size=0, eoa=2144
    major: Invalid arguments to routine
    minor: Address overflowed
MPI-process 0:
  #000: H5Dio.c line 271 in H5Dwrite(): can't prepare for writing data
    major: Dataset
    minor: Write failed
  #001: H5Dio.c line 352 in H5D__pre_write(): can't write data
    major: Dataset
    minor: Write failed
  #002: H5Dio.c line 789 in H5D__write(): can't write data

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Error in routine  (1):
     
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

     stopping ...
    major: Dataset
    minor: Write failed
  #003: H5Dmpio.c line 529 in H5D__contig_collective_write(): couldn't finish shared collective MPI-IO
    major: Low-level I/O
    minor: Write failed
  #004: H5Dmpio.c line 1399 in H5D__inter_collective_io(): couldn't finish collective MPI-IO
    major: Low-level I/O
    minor: Can't get value
  #005: H5Dmpio.c line 1443 in H5D__final_collective_io(): optimized write failed
    major: Dataset
    minor: Write failed
  #006: H5Dmpio.c line 297 in H5D__mpio_select_write(): can't finish collective parallel write
    major: Low-level I/O
    minor: Write failed
  #007: H5Fio.c line 171 in H5F_block_write(): write through metadata accumulator failed
    major: Low-level I/O
    minor: Write failed
  #008: H5Faccum.c line 825 in H5F__accum_write(): file write failed
    major: Low-level I/O
    minor: Write failed
  #009: H5FDint.c line 256 in H5FD_write(): addr overflow, addr = 18446744073709551615, size=0, eoa=2144
    major: Invalid arguments to routine
    minor: Address overflowed

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     Error in routine  (1):
     
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

     stopping ...
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 1
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0
